{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyMljLOb0DOkCho24xriu2zP"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n!pip install flask pyngrok torch transformers vllm\n\nimport os\nimport threading\nimport time\nfrom flask import Flask, request, jsonify\nfrom pyngrok import ngrok, conf\nfrom kaggle_secrets import UserSecretsClient\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom vllm import LLM, SamplingParams\nimport gc\nimport re\nimport psutil\n\n# Retrieve secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nngrok_token = user_secrets.get_secret(\"NGROK_TOKEN\")\n\n# Set Ngrok authtoken\nconf.get_default().auth_token = ngrok_token\n\n# Set environment variables to reduce memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_TOKEN\"] = hf_token\n\n# Set CUDA_VISIBLE_DEVICES to map cuda:0 to GPU 1 and cuda:1 to GPU 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,0\"\n\n# Clear GPU memory\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(\"Cleared GPU memory.\")\n\nclear_gpu_memory()\n\n# Load embedding model on cuda:1 (physical GPU 0)\nembedding_model_name = \"BAAI/bge-en-icl\"\nembedding_device = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\")\ntry:\n    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name, token=hf_token)\n    embedding_model = AutoModel.from_pretrained(\n        embedding_model_name,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n        token=hf_token\n    ).eval().to(embedding_device)\nexcept Exception as e:\n    print(f\"Failed to load embedding model: {e}\")\n    raise\n\n# Load generation model with VLLM on cuda:0 (physical GPU 1)\nplan_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\ntry:\n    plan_llm = LLM(\n        model=plan_model_name,\n        tensor_parallel_size=1,\n        dtype=\"float16\",\n        gpu_memory_utilization=0.85,\n        max_model_len=4096\n    )\n    plan_tokenizer = AutoTokenizer.from_pretrained(plan_model_name, token=hf_token)\n    if plan_tokenizer.pad_token is None:\n        plan_tokenizer.pad_token = plan_tokenizer.eos_token\nexcept Exception as e:\n    print(f\"Failed to load generation model: {e}\")\n    raise\n\n# Flask app\napp = Flask(__name__)\n\n@app.route('/embed', methods=['POST'])\ndef embed():\n    try:\n        data = request.json\n        texts = data.get('texts', [])\n        max_length = data.get('max_length', 256)\n        clear_gpu_memory()\n        inputs = embedding_tokenizer(\n            [\"query: \" + text for text in texts],\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=True,\n            max_length=max_length\n        )\n        inputs = {k: v.to(embedding_device) for k, v in inputs.items()}\n        with torch.no_grad(), torch.amp.autocast('cuda'):\n            outputs = embedding_model(**inputs)\n            embeddings = outputs.last_hidden_state[:, 0]\n            norm_embed = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        embeddings_list = norm_embed.cpu().tolist()\n        return jsonify({'embeddings': embeddings_list})\n    except Exception as e:\n        print(f\"Embedding error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate', methods=['POST'])\ndef generate():\n    try:\n        data = request.json\n        prompt = data.get('prompt', '')\n        sampling_params = data.get('sampling_params', {})\n        # Apply chat template\n        formatted_prompt = plan_tokenizer.apply_chat_template(\n            [{\"role\": \"system\", \"content\": \"You are a doctor speaking to a parent.\"},\n             {\"role\": \"user\", \"content\": prompt}],\n            tokenize=False\n        )\n        # Generate text\n        outputs = plan_llm.generate([formatted_prompt], SamplingParams(**sampling_params))\n        generated_text = outputs[0].outputs[0].text.strip()\n        print(f\"Raw vLLM output: {repr(generated_text)}\")\n        # Robustly strip assistant prefix, role tags, and any vLLM artifacts\n        generated_text = re.sub(r'^(?:assistant|user|system|<\\|[^>]*\\|>)+[\\s\\n]*', '', generated_text, flags=re.MULTILINE | re.IGNORECASE)\n        generated_text = re.sub(r'\\n\\s*\\n+', '\\n', generated_text).strip()\n        print(f\"Processed generated text: {repr(generated_text)}\")\n        return jsonify({'generated_text': generated_text})\n    except Exception as e:\n        print(f\"Generation error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/status', methods=['GET'])\ndef status():\n    try:\n        memory = psutil.virtual_memory()\n        gpu_memory = torch.cuda.memory_allocated(embedding_device) / 1024**2 if torch.cuda.is_available() else 0\n        return jsonify({\n            'status': 'running',\n            'memory_percent': memory.percent,\n            'gpu_memory_mb': gpu_memory\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n# Start Flask app in a separate thread\ndef run_flask():\n    app.run(host='0.0.0.0', port=5000)\n\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.start()\n\n# Start Ngrok tunnel\npublic_url = ngrok.connect(5000)\nprint(f\"Ngrok Tunnel URL: {public_url}\")\n\n# Keep the notebook running and log status\nwhile True:\n    try:\n        memory = psutil.virtual_memory()\n        print(f\"Server status: Memory usage {memory.percent}%\")\n        time.sleep(60)\n    except Exception as e:\n        print(f\"Status check error: {e}\")\n        time.sleep(60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T08:27:34.303468Z","iopub.execute_input":"2025-04-27T08:27:34.304061Z","iopub.status.idle":"2025-04-27T09:05:55.149485Z","shell.execute_reply.started":"2025-04-27T08:27:34.304037Z","shell.execute_reply":"2025-04-27T09:05:55.138456Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\nCollecting pyngrok\n  Downloading pyngrok-7.2.5-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting vllm\n  Downloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\nCollecting blake3 (from vllm)\n  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (3.20.3)\nCollecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.16)\nRequirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.61.1)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.3)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nCollecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\nCollecting llguidance<0.8.0,>=0.7.9 (from vllm)\n  Downloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting outlines==0.1.11 (from vllm)\n  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\nCollecting lark==1.2.2 (from vllm)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting xgrammar==0.1.18 (from vllm)\n  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting partial-json-parser (from vllm)\n  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\nCollecting msgspec (from vllm)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf>=0.13.0 (from vllm)\n  Downloading gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\nCollecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nCollecting compressed-tensors==0.9.3 (from vllm)\n  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\nCollecting depyf==0.18.0 (from vllm)\n  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nCollecting watchfiles (from vllm)\n  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nCollecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n  Downloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl.metadata (1.2 kB)\nCollecting numba==0.61.2 (from vllm)\n  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n  Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting torch\n  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting torchaudio==2.6.0 (from vllm)\n  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting torchvision==0.21.0 (from vllm)\n  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\nCollecting xformers==0.0.29.post2 (from vllm)\n  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting triton==3.2.0 (from torch)\n  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting astor (from depyf==0.18.0->vllm)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting interegular (from outlines==0.1.11->vllm)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nCollecting diskcache (from outlines==0.1.11->vllm)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\nCollecting pycountry (from outlines==0.1.11->vllm)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting airportsdata (from outlines==0.1.11->vllm)\n  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\nCollecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nCollecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nCollecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nCollecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->vllm)\n  Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\nCollecting importlib_metadata (from vllm)\n  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.67.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.5.0)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.19.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\nCollecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rich_toolkit-0.14.3-py3-none-any.whl.metadata (999 bytes)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.22.3)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nDownloading pyngrok-7.2.5-py3-none-any.whl (23 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl (294.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.1/294.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m53.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\nDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m114.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gguf-0.16.2-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\nDownloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\nDownloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl (5.4 kB)\nDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\nDownloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl (67.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\nDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\nDownloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading rich_toolkit-0.14.3-py3-none-any.whl (24 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, blake3, uvloop, uvicorn, python-multipart, python-dotenv, pyngrok, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, importlib_metadata, httptools, hf-xet, diskcache, astor, airportsdata, watchfiles, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, depyf, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, ray, outlines_core, opentelemetry-sdk, fastapi-cli, torchaudio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, mistral_common, xgrammar, xformers, torchvision, outlines, numba, gguf, compressed-tensors, vllm\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: importlib_metadata\n    Found existing installation: importlib_metadata 8.6.1\n    Uninstalling importlib_metadata-8.6.1:\n      Successfully uninstalled importlib_metadata-8.6.1\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.16.0\n    Uninstalling opentelemetry-api-1.16.0:\n      Successfully uninstalled opentelemetry-api-1.16.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: ray\n    Found existing installation: ray 2.44.1\n    Uninstalling ray-2.44.1:\n      Successfully uninstalled ray-2.44.1\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.16.0\n    Uninstalling opentelemetry-sdk-1.16.0:\n      Successfully uninstalled opentelemetry-sdk-1.16.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu124\n    Uninstalling torchaudio-2.5.1+cu124:\n      Successfully uninstalled torchaudio-2.5.1+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 blake3-1.0.4 compressed-tensors-0.9.3 depyf-0.18.0 diskcache-5.6.3 fastapi-0.115.12 fastapi-cli-0.0.7 gguf-0.16.2 hf-xet-1.0.5 httptools-0.6.4 importlib_metadata-8.0.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.19 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.3 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 pycountry-24.6.1 pyngrok-7.2.5 python-dotenv-1.1.0 python-multipart-0.0.20 ray-2.43.0 rich-toolkit-0.14.3 starlette-0.46.2 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0 uvicorn-0.34.2 uvloop-0.21.0 vllm-0.8.4 watchfiles-1.0.5 xformers-0.0.29.post2 xgrammar-0.1.18\nINFO 04-27 08:30:55 [__init__.py:239] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-04-27 08:30:57.730626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745742657.971577      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745742658.040087      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Cleared GPU memory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ee578dda2f452da3ae3f3f74b373c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd7a9489daf457abd05831a0318bd6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea96082e41db4de991fb55d65ec60e9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e66d79b2654c94a54a25957ab5e159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/640 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac613d0ca50f4b5ab5c2def77aa28715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e3af1a171284fa4a67a49051a35c642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/22.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1e82557f5f74046837cbcf1c7af8eb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad7a27f5618a4d1498b2f2e3b4361b58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/8.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd9cac25c72148a5b8c056f5972809e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912670a993cd47a38a8b905932f79d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/9.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8934a769b0444dcea831189b4de6912c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e340b58a3c74e048e20e5bfa2787510"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48247fabdfd14458a7ca0248640aad36"}},"metadata":{}},{"name":"stdout","text":"WARNING 04-27 08:34:58 [config.py:2836] Casting torch.bfloat16 to torch.float16.\nINFO 04-27 08:35:19 [config.py:689] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nWARNING 04-27 08:35:19 [arg_utils.py:1731] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nINFO 04-27 08:35:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"317b4d067a1a48a7bd08259338f7034d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ba416d249640da84102cbafceb1ba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d63fa8c2578451b919c77449f827598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71966cfe89a648afa28d62bc5d9f4a2c"}},"metadata":{}},{"name":"stdout","text":"INFO 04-27 08:35:21 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 04-27 08:35:21 [cuda.py:289] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"[W427 08:35:32.605691632 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 04-27 08:35:42 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-27 08:35:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n","output_type":"stream"},{"name":"stderr","text":"[W427 08:35:42.616360965 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 04-27 08:35:42 [weight_utils.py:265] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d50af078955a45af9e00c9a0e84b4884"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b49afd496c472998a0c1c943cc384e"}},"metadata":{}},{"name":"stdout","text":"INFO 04-27 08:35:57 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.2-3B-Instruct: 14.969462 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060800fb76ad4b4c9459dab8e84c18a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b3c4aeebcb4618afbba980e4c066a6"}},"metadata":{}},{"name":"stdout","text":"INFO 04-27 08:36:10 [loader.py:458] Loading weights took 12.37 seconds\nINFO 04-27 08:36:11 [model_runner.py:1146] Model loading took 6.0155 GiB and 28.077815 seconds\nINFO 04-27 08:36:13 [worker.py:267] Memory profiling takes 2.08 seconds\nINFO 04-27 08:36:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.85) = 12.53GiB\nINFO 04-27 08:36:13 [worker.py:267] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 5.27GiB.\nINFO 04-27 08:36:14 [executor_base.py:112] # cuda blocks: 3086, # CPU blocks: 2340\nINFO 04-27 08:36:14 [executor_base.py:117] Maximum concurrency for 4096 tokens per request: 12.05x\nINFO 04-27 08:36:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49826e3a97304ceabb0cd3b5f8f7731f"}},"metadata":{}},{"name":"stdout","text":"INFO 04-27 08:37:01 [model_runner.py:1598] Graph capturing finished in 42 secs, took 0.19 GiB\nINFO 04-27 08:37:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 50.55 seconds\n * Serving Flask app '__main__'\n * Debug mode: off\nNgrok Tunnel URL: NgrokTunnel: \"https://c833-35-192-157-204.ngrok-free.app\" -> \"http://localhost:5000\"\nServer status: Memory usage 33.5%\nServer status: Memory usage 33.7%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.7%\nServer status: Memory usage 33.7%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.7%\nServer status: Memory usage 33.5%\nServer status: Memory usage 33.7%\nServer status: Memory usage 33.7%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.6%\nServer status: Memory usage 33.7%\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9131c1c087914621a4a7939f3593cf6e"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: '<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your child, we recommend administering 300-450 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. For pain relief, 150-300 mg of Ibuprofen can be given every 6-8 hours.\\n\\nCaution! Avoid giving alcohol to your child, as it may increase the risk of liver damage with Acetaminophen. Also, be cautious when using Ibuprofen with hypertension, as it may worsen blood pressure. Additionally, avoid consuming alcohol while taking Ibuprofen to minimize the risk of stomach bleeding. Consult a doctor!'\nProcessed generated text: 'Hello! For your child, we recommend administering 300-450 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. For pain relief, 150-300 mg of Ibuprofen can be given every 6-8 hours.\\nCaution! Avoid giving alcohol to your child, as it may increase the risk of liver damage with Acetaminophen. Also, be cautious when using Ibuprofen with hypertension, as it may worsen blood pressure. Additionally, avoid consuming alcohol while taking Ibuprofen to minimize the risk of stomach bleeding. Consult a doctor!'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a4db805a1ce41d3801588c632a780ec"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: '<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your child, I recommend administering 300-450 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. For pain relief, you can also consider 150-300 mg of Ibuprofen every 6-8 hours.\\n\\nCaution! Avoid giving alcohol to your child, as it may increase the risk of liver damage with Acetaminophen. Also, be cautious when using Ibuprofen with hypertension, as it may worsen blood pressure. Additionally, avoid consuming alcohol while taking Ibuprofen to minimize the risk of stomach bleeding. Consult a doctor!'\nProcessed generated text: 'Hello! For your child, I recommend administering 300-450 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. For pain relief, you can also consider 150-300 mg of Ibuprofen every 6-8 hours.\\nCaution! Avoid giving alcohol to your child, as it may increase the risk of liver damage with Acetaminophen. Also, be cautious when using Ibuprofen with hypertension, as it may worsen blood pressure. Additionally, avoid consuming alcohol while taking Ibuprofen to minimize the risk of stomach bleeding. Consult a doctor!'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e33e7094de946f5905acc11836bc2c8"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: '<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your child, we recommend administering 300-450 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. For Ibuprofen, give 150-300 mg every 6-8 hours. Follow doctor’s advice!\\n\\nCaution! Avoid giving your child alcohol and avoid giving Ibuprofen to children with hypertension. Also, avoid consuming alcohol while taking Acetaminophen due to liver risk. Consult a doctor!'\nProcessed generated text: 'Hello! For your child, we recommend administering 300-450 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. For Ibuprofen, give 150-300 mg every 6-8 hours. Follow doctor’s advice!\\nCaution! Avoid giving your child alcohol and avoid giving Ibuprofen to children with hypertension. Also, avoid consuming alcohol while taking Acetaminophen due to liver risk. Consult a doctor!'\nServer status: Memory usage 34.2%\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb42bd4d7854ebebd0bb89d22678885"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: '<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your child, we recommend taking 250-375 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. Follow doctor’s advice!\\n\\nCaution! Avoid giving aspirin to your child, especially with alcohol consumption, as it may increase the risk of stomach bleeding. Also, avoid consuming alcohol while taking Acetaminophen, as it may worsen liver damage. Consult a doctor!'\nProcessed generated text: 'Hello! For your child, we recommend taking 250-375 mg of Acetaminophen every 4-6 hours, not to exceed 5 doses in a 24-hour period. Follow doctor’s advice!\\nCaution! Avoid giving aspirin to your child, especially with alcohol consumption, as it may increase the risk of stomach bleeding. Also, avoid consuming alcohol while taking Acetaminophen, as it may worsen liver damage. Consult a doctor!'\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\nServer status: Memory usage 34.2%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9baf74c93741e280472fe2c09e2ce8"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: '<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your cough, I recommend taking 30 mL of Guaifenesin 3 times a day. However, since the dosage for Dextromethorphan is not specified, I will not prescribe it. Please consult a doctor for further guidance.\\n\\nCaution! Avoid consuming alcohol while taking Guaifenesin, as it may increase the risk of sedation. No interactions with food or other medications have been noted. Consult a doctor! Follow doctor’s advice!'\nProcessed generated text: 'Hello! For your cough, I recommend taking 30 mL of Guaifenesin 3 times a day. However, since the dosage for Dextromethorphan is not specified, I will not prescribe it. Please consult a doctor for further guidance.\\nCaution! Avoid consuming alcohol while taking Guaifenesin, as it may increase the risk of sedation. No interactions with food or other medications have been noted. Consult a doctor! Follow doctor’s advice!'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31cdde1872844970a55570062b86b845"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: '<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your cough, I recommend taking 30 mL of Guaifenesin 3 times a day. However, since the dosage for Dextromethorphan is not specified, I will not prescribe it. Please consult a doctor for further guidance.\\n\\nCaution! Avoid consuming alcohol while taking Guaifenesin, as it may increase the risk of sedation. No interactions with food or other medications have been noted. Consult a doctor! Follow doctor’s advice!'\nProcessed generated text: 'Hello! For your cough, I recommend taking 30 mL of Guaifenesin 3 times a day. However, since the dosage for Dextromethorphan is not specified, I will not prescribe it. Please consult a doctor for further guidance.\\nCaution! Avoid consuming alcohol while taking Guaifenesin, as it may increase the risk of sedation. No interactions with food or other medications have been noted. Consult a doctor! Follow doctor’s advice!'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a53caa9c35f742d78d11654ea31607fd"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: \"<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For your cough, I recommend taking 30mg of Dextromethorphan and 400mg of Guaifenesin every 12 hours as needed. Follow doctor’s advice!\\n\\nCaution! Avoid consuming alcohol while taking Dextromethorphan due to increased sedation risk. No interactions with food or other medications have been noted. Consult a doctor! \\n\\nPlease note: Since Dextromethorphan dosage isn't specified, please consult a doctor for personalized dosage.\"\nProcessed generated text: \"Hello! For your cough, I recommend taking 30mg of Dextromethorphan and 400mg of Guaifenesin every 12 hours as needed. Follow doctor’s advice!\\nCaution! Avoid consuming alcohol while taking Dextromethorphan due to increased sedation risk. No interactions with food or other medications have been noted. Consult a doctor! \\nPlease note: Since Dextromethorphan dosage isn't specified, please consult a doctor for personalized dosage.\"\nServer status: Memory usage 34.2%\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\nCleared GPU memory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812506836b5f48abbc2b29674b929ffb"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: \"<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For anxiety management, I recommend starting with a low dose of Lorazepam 0.5mg 3 times a day and Diazepam 2mg 2 times a day. Follow doctor’s advice!\\n\\nCaution! No interactions with food or alcohol have been reported. However, it's essential to inform your doctor about any medications or supplements you're taking before starting this treatment. Consult a doctor! \\n\\n(Note: Please consult a doctor for personalized dosage and treatment as the provided information is based on general data and may not be specific to your individual case.)\"\nProcessed generated text: \"Hello! For anxiety management, I recommend starting with a low dose of Lorazepam 0.5mg 3 times a day and Diazepam 2mg 2 times a day. Follow doctor’s advice!\\nCaution! No interactions with food or alcohol have been reported. However, it's essential to inform your doctor about any medications or supplements you're taking before starting this treatment. Consult a doctor! \\n(Note: Please consult a doctor for personalized dosage and treatment as the provided information is based on general data and may not be specific to your individual case.)\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b59f24f9a740ecbf628b2fb8d6e708"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: \"<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! Due to the lack of specific dosage information for Lorazepam at 50 years old, I recommend starting with a low dose of 0.5-1.0 mg orally, 3 times a day. For Diazepam, a starting dose of 2.5-5.0 mg orally, 2-3 times a day, may be considered. Follow doctor’s advice!\\n\\nCaution! No interactions have been found between Lorazepam and Diazepam with food or alcohol. However, it's essential to inform your doctor about any medications or substances you're taking, as they may still interact with these medications. Consult a doctor!\"\nProcessed generated text: \"Hello! Due to the lack of specific dosage information for Lorazepam at 50 years old, I recommend starting with a low dose of 0.5-1.0 mg orally, 3 times a day. For Diazepam, a starting dose of 2.5-5.0 mg orally, 2-3 times a day, may be considered. Follow doctor’s advice!\\nCaution! No interactions have been found between Lorazepam and Diazepam with food or alcohol. However, it's essential to inform your doctor about any medications or substances you're taking, as they may still interact with these medications. Consult a doctor!\"\nServer status: Memory usage 34.3%\nCleared GPU memory.\nCleared GPU memory.\nServer status: Memory usage 34.2%\nCleared GPU memory.\nCleared GPU memory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f91979b17a44bda1f0061a58ebce5a"}},"metadata":{}},{"name":"stdout","text":"Raw vLLM output: \"<|start_header_id|>assistant<|end_header_id|>\\n\\nHello! For the patient, I recommend starting with a low dose of Alprazolam, 0.25mg orally, 2-3 times a day, and Clonazepam, 1mg orally, 2-3 times a day. Follow doctor’s advice!\\n\\nCaution! No significant food or alcohol interactions were found for this patient. However, it's essential to inform the patient to avoid consuming grapefruit products and grapefruit juice, as they may interact with these medications. Consult a doctor!\"\nProcessed generated text: \"Hello! For the patient, I recommend starting with a low dose of Alprazolam, 0.25mg orally, 2-3 times a day, and Clonazepam, 1mg orally, 2-3 times a day. Follow doctor’s advice!\\nCaution! No significant food or alcohol interactions were found for this patient. However, it's essential to inform the patient to avoid consuming grapefruit products and grapefruit juice, as they may interact with these medications. Consult a doctor!\"\nServer status: Memory usage 34.3%\nServer status: Memory usage 34.3%\nServer status: Memory usage 34.2%\nServer status: Memory usage 34.2%\nServer status: Memory usage 34.2%\nServer status: Memory usage 34.2%\nServer status: Memory usage 34.2%\nServer status: Memory usage 34.2%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/964997745.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Server status: Memory usage {memory.percent}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Status check error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}